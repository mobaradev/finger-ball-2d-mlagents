{
    "name": "root",
    "gauges": {
        "AgentBall.Policy.Entropy.mean": {
            "value": 0.5350512266159058,
            "min": 0.5302100777626038,
            "max": 0.6238176822662354,
            "count": 80
        },
        "AgentBall.Policy.Entropy.sum": {
            "value": 5657.6318359375,
            "min": 3731.5654296875,
            "max": 7003.41748046875,
            "count": 80
        },
        "AgentBall.Environment.EpisodeLength.mean": {
            "value": 186.67741935483872,
            "min": 32.06666666666667,
            "max": 270.4117647058824,
            "count": 80
        },
        "AgentBall.Environment.EpisodeLength.sum": {
            "value": 11574.0,
            "min": 962.0,
            "max": 13880.0,
            "count": 80
        },
        "AgentBall.Step.mean": {
            "value": 5739958.0,
            "min": 4949974.0,
            "max": 5739958.0,
            "count": 80
        },
        "AgentBall.Step.sum": {
            "value": 5739958.0,
            "min": 4949974.0,
            "max": 5739958.0,
            "count": 80
        },
        "AgentBall.Policy.ExtrinsicValueEstimate.mean": {
            "value": -7.749329566955566,
            "min": -17.783935546875,
            "max": -4.275243759155273,
            "count": 80
        },
        "AgentBall.Policy.ExtrinsicValueEstimate.sum": {
            "value": -1433.6259765625,
            "min": -3076.620849609375,
            "max": -631.3978271484375,
            "count": 80
        },
        "AgentBall.Policy.CuriosityValueEstimate.mean": {
            "value": 0.524742603302002,
            "min": 0.49226298928260803,
            "max": 0.6291132569313049,
            "count": 80
        },
        "AgentBall.Policy.CuriosityValueEstimate.sum": {
            "value": 97.07737731933594,
            "min": 27.680984497070312,
            "max": 116.94432830810547,
            "count": 80
        },
        "AgentBall.Environment.CumulativeReward.mean": {
            "value": -22.8671875,
            "min": -67.57608695652173,
            "max": 18.6,
            "count": 80
        },
        "AgentBall.Environment.CumulativeReward.sum": {
            "value": -1463.5,
            "min": -3108.5,
            "max": 558.0,
            "count": 80
        },
        "AgentBall.Policy.ExtrinsicReward.mean": {
            "value": -22.8671875,
            "min": -67.57608695652173,
            "max": 18.6,
            "count": 80
        },
        "AgentBall.Policy.ExtrinsicReward.sum": {
            "value": -1463.5,
            "min": -3108.5,
            "max": 558.0,
            "count": 80
        },
        "AgentBall.Policy.CuriosityReward.mean": {
            "value": 1.0104744019918144,
            "min": 0.0,
            "max": 1.511396464836948,
            "count": 80
        },
        "AgentBall.Policy.CuriosityReward.sum": {
            "value": 64.67036172747612,
            "min": 0.0,
            "max": 77.5750364176929,
            "count": 80
        },
        "AgentBall.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 80
        },
        "AgentBall.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 80
        },
        "AgentBall.Losses.PolicyLoss.mean": {
            "value": 0.04370685089573574,
            "min": 0.03615228131723901,
            "max": 0.06033342770533636,
            "count": 79
        },
        "AgentBall.Losses.PolicyLoss.sum": {
            "value": 0.21853425447867872,
            "min": 0.08828608556211912,
            "max": 0.3016671385266818,
            "count": 79
        },
        "AgentBall.Losses.ValueLoss.mean": {
            "value": 99.7211424191793,
            "min": 46.53835527102152,
            "max": 165.47447865251183,
            "count": 79
        },
        "AgentBall.Losses.ValueLoss.sum": {
            "value": 498.6057120958965,
            "min": 186.1534210840861,
            "max": 640.8565467198689,
            "count": 79
        },
        "AgentBall.Policy.LearningRate.mean": {
            "value": 0.00019961763021785167,
            "min": 0.00019961763021785167,
            "max": 0.00019966965929850376,
            "count": 79
        },
        "AgentBall.Policy.LearningRate.sum": {
            "value": 0.0009980881510892583,
            "min": 0.0003993393185970075,
            "max": 0.0009983418201624235,
            "count": 79
        },
        "AgentBall.Policy.Epsilon.mean": {
            "value": 0.1998088150133333,
            "min": 0.1998088150133333,
            "max": 0.19983482956666668,
            "count": 79
        },
        "AgentBall.Policy.Epsilon.sum": {
            "value": 0.9990440750666666,
            "min": 0.39966965913333335,
            "max": 0.9991709096666667,
            "count": 79
        },
        "AgentBall.Policy.Beta.mean": {
            "value": 0.014971341370498664,
            "min": 0.014971341370498664,
            "max": 0.01497524095204333,
            "count": 79
        },
        "AgentBall.Policy.Beta.sum": {
            "value": 0.07485670685249332,
            "min": 0.02995048190408666,
            "max": 0.07487571935903332,
            "count": 79
        },
        "AgentBall.Losses.CuriosityForwardLoss.mean": {
            "value": 0.05242539867758751,
            "min": 0.048841281599986054,
            "max": 0.06792049018153246,
            "count": 79
        },
        "AgentBall.Losses.CuriosityForwardLoss.sum": {
            "value": 0.26212699338793755,
            "min": 0.1358409803630649,
            "max": 0.3094421889012059,
            "count": 79
        },
        "AgentBall.Losses.CuriosityInverseLoss.mean": {
            "value": 0.18800423480570316,
            "min": 0.15286602688332399,
            "max": 0.23749271340236283,
            "count": 79
        },
        "AgentBall.Losses.CuriosityInverseLoss.sum": {
            "value": 0.9400211740285158,
            "min": 0.47498542680472566,
            "max": 1.1551577157030504,
            "count": 79
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1715488194",
        "python_version": "3.9.0 | packaged by conda-forge | (default, Nov 26 2020, 07:53:15) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "\\\\?\\C:\\Users\\mobara\\miniforge3\\envs\\mlagentsv20\\Scripts\\mlagents-learn AgentBall5.yaml --run-id=simpleagentx8-1 --resume",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.7.1+cu110",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1715490876"
    },
    "total": 2682.8184580999996,
    "count": 1,
    "self": 0.012017399999876943,
    "children": {
        "run_training.setup": {
            "total": 0.11151630000000012,
            "count": 1,
            "self": 0.11151630000000012
        },
        "TrainerController.start_learning": {
            "total": 2682.6949243999998,
            "count": 1,
            "self": 2.6355252999883305,
            "children": {
                "TrainerController._reset_env": {
                    "total": 10.2458561,
                    "count": 1,
                    "self": 10.2458561
                },
                "TrainerController.advance": {
                    "total": 2669.712666800012,
                    "count": 97401,
                    "self": 2.5068181999986336,
                    "children": {
                        "env_step": {
                            "total": 2056.367012700004,
                            "count": 97401,
                            "self": 1455.2837729000144,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 599.2140577999764,
                                    "count": 97401,
                                    "self": 13.505360499917742,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 585.7086973000587,
                                            "count": 192410,
                                            "self": 585.7086973000587
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 1.8691820000134083,
                                    "count": 97400,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 2670.1225259000626,
                                            "count": 97400,
                                            "is_parallel": true,
                                            "self": 1393.4559871000529,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0018546999999990987,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.000685600000000619,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0011690999999984797,
                                                            "count": 4,
                                                            "is_parallel": true,
                                                            "self": 0.0011690999999984797
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 1276.6646841000097,
                                                    "count": 97400,
                                                    "is_parallel": true,
                                                    "self": 16.791770400069026,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 21.056423600002056,
                                                            "count": 97400,
                                                            "is_parallel": true,
                                                            "self": 21.056423600002056
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 1180.5966035000004,
                                                            "count": 97400,
                                                            "is_parallel": true,
                                                            "self": 1180.5966035000004
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 58.21988659993823,
                                                            "count": 194800,
                                                            "is_parallel": true,
                                                            "self": 34.98910600006876,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 23.230780599869465,
                                                                    "count": 389600,
                                                                    "is_parallel": true,
                                                                    "self": 23.230780599869465
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 610.8388359000089,
                            "count": 97400,
                            "self": 3.667937699995946,
                            "children": {
                                "process_trajectory": {
                                    "total": 139.48048510001297,
                                    "count": 97400,
                                    "self": 139.300867600013,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 0.17961749999997778,
                                            "count": 2,
                                            "self": 0.17961749999997778
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 467.6904131,
                                    "count": 371,
                                    "self": 340.2593243000136,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 127.43108879998641,
                                            "count": 9003,
                                            "self": 127.43108879998641
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 2.1999999262334313e-06,
                    "count": 1,
                    "self": 2.1999999262334313e-06
                },
                "TrainerController._save_models": {
                    "total": 0.10087399999974878,
                    "count": 1,
                    "self": 0.01720069999964835,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.08367330000010043,
                            "count": 1,
                            "self": 0.08367330000010043
                        }
                    }
                }
            }
        }
    }
}